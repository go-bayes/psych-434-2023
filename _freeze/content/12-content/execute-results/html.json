{
  "hash": "a1967af522ef18e027ce0cd78564fb19",
  "result": {
    "markdown": "---\ntitle: \"Template: Stratified Causal Estimatation\"\ndate: \"2023-JUNE-06\"\nauthor:\n    name: Joseph Bulbulia\n    orcid: 0000-0002-5861-2056\n    affiliation: Victoria University of Wellington, New Zealand\n    email: joseph.bulbulia@vuw.ac.nz\n    corresponding: yes\nbibliography: references.bib\nformat: \n  html:\n    html-math-method: katex\n    warnings: false\n    error: false\n    messages: false\n    highlight-style: solarized\n    code-tools:\n      source: true\n      toggle: false\n      caption: none\n---\n\n\n\nYour step-by-step guide to reporting is as follows: \n\n\n## Intoduction\n\nAnswer the following:\n\n-   State the Question: is my question clearly stated? If not, state it.\n-   Relevance of the Question: Have I explained its importance? If not, explain.\n-   Subgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\n-   Causality of the Question: Is my question causal? Briefly explain what this means with reference to the potential outcomes framework.\n-   State how you will use time-series data to address causality.\n-   Define your exposure.\n-   Define your outcome(s)\n-   Explain how the the exposure and outcome is relevant to your question.\n-   Define your causal estimand (see: lecture 9). Hint: it is ATE_g\\_risk difference = E\\[Y(1)-(0)\\|G,L\\], where G is your multiple-group indicator and L is your set of baseline confounders.\n\n## Method\n\n-   Consider any ethical implications.\n-   Explain the sample. Provide descriptive statistics\n-   Discuss inclusion criteria.\n-   Discuss how your sample relates to the \"source population\" (lecture 9.)\n-   Explain NZAVS measures. State the questions used in the items\n-   In your own words describe how the data meet the following assumptions required for causal inference:\n-   Positivity: Can we intervene on the exposure at all levels of the covariates? Use the code I provided to test whether there is change in the exposure from the baseline in the source population(s)\n-   Consistency: Can I interpret what it means to intervene on the exposure?\n-   Exchangeability: Are different versions of the exposure conditionally exchangeable given measured baseline confounders? This requires stating baseline confounders and explaining how they may be related to both the exposure and outcome. As part of this, **you must explain why the baseline measure of your exposure and outcome are included as potential confounders.**\n-   Note: Unmeasured Confounders: Does previous science suggest the presence of unmeasured confounders? (e.g. childhood exposures that are not measured).\n-   Draw a causal diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding?\n-   Measurement Error: Have I described potential biases from measurement errors? Return to lecture 11.\n-   State that you do not have missing data in this synthetic dataset, but that ordinarily missing data would need to be handled.\n-   State what your estimator will be. Note I've given you the following text to modify:\n\n> The Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\n\n> Step 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\n\n> Step 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\n\n> Step 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\n\n> Step 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\n\n> Step 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\nAlso see the appendix [here](https://go-bayes.github.io/psych-434-2023/content/09-content.html#comprehensive-checklist-for-detailed-reporting-of-a-causal-inferenctial-study-e.g.-assessment-3-option-2)\n\n-   State what E-values are and how you will use them to clarify the risk of unmeasured confounding.\n\n## Results\n\n-   Use the scripts I have provided as a template for your analysis.\n-   Propensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations: e.g.: `A ~ x1 + x2 + x3 + ....` using logistic regression, all continuous predictors were transformed to z-scores\n    -   WeightIt Package Utilisation: Explicitly mention the use of the 'WeightIt' package in R, including any specific options or parameters used in the propensity score estimation process [@greifer2023a].\n    -   Report if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as 'ebal', 'energy', and 'ps'.\n    -   If your exposure is continuous only the 'energy' option was used for propensity score estimation.\n    -   Subgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\n    -   Covariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting. The script will generate these plots.\n    -   Weighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit. The script I gave you will generate this information\n\nExample:\n\n> We estimated propensity scores by fitting a model for the exposure A as it is predicted by the set of baseline covariates defined by L. Because we are interested in effect modification by group, we fit different propensity score models for within strata of G using the `subgroup` command of the `WeightIt` package. Thus the propensity score is the the probability of receiving a value of a treatment (A=a) conditional on the covariates L, and stratum within G. We compared balance using the following methods of weighting: \"ebal\" or entropy balancing, \"energy\" or energy balancing, and \"ps\" or traditional inverse probability of weighting balancing. Of these methods \"ebal\" performed the best. Table X and Figure Y present the results of the ebalancing method.\n\n-   Interpretation of Propensity Scores: we interpret the proposensity scores as yeilding good balance across the exposure conditions.\n\n-   Outcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation. Example\n\n> We fit a linear model using maximum likelihood estimation with the outcome Y predicted by the exposure A. We interacted the exposure with all baseline confounders L. Continuous baseline confounders were converted to z-scores, whereas categorical exposures were not. Also interacted with all baseline confounders was a term for the subgroup interactoin. This allowed uas to flexibily fit non-linearities for the modification of the effect of the exposure within levels of the levels of the cultural group strata of interest. We note that model coefficients have no interpretation in this context so are not reported. The remaining steps of Doubly-Robust estimation were performed as outlined in the *Method* section. We calculated confidence intervals and standard errors, using the `clarify` package in R, which relies on simulation based inference for these quantities of interest [@greifer2023]\n\n-   Report the causal estimates.\n    -   ATE contrasts for groups in setting the exposure to for each group in setting level A = a and A = a\\*\n    -   differences between groups in the magnitude of the effects. (ATE_group 1 - ATE_group_2)\n-   Report the E-value: how sensitive are your results to unmeasured confounding? Hint: see the code below. I've substantially automated this task.\n\n\n## Discussion\n\nMake sure to hit these points:\n\nConsider the following ideas about what to discuss in one's findings. The order of exposition might be different.\n\n1.  **Summary of results**: What did you find?\n\n2.  **Interpretation of E-values:** Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\n\n3.  **Causal Effect Interpretation:** What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\n\n4.  **Comparison of Subgroups:** Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\n\n5.  **Uncertainty and Confidence Intervals:** Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\n\n6.  **Generalisability and Transportability:** Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\n\n7.  **Assumptions and Limitations:** Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed.\n\n8.  **Theoretical Relevance**: How are these findings relevant to existing theories.\n\n9.  **Replication and Future Research:** Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\n\n10. **Real-world Implications:** Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research.\n\n## Example anlaysis (week 10)\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Before running this source code, make sure to update to the current version of R, and to update all existing packages.\n\n# functions\nsource(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\n\n\n# experimental functions (more functions)\nsource(\n  \"https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R\"\n)\n\n\n#  If you haven't already, you should have created a folder called \"data\", in your Rstudio project. If not, download this file, add it to your the folder called \"data\" in your Rstudio project. # \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n```\n:::\n\n\n### Import data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data.\n\nnzavs_synth <- arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n```\n:::\n\n\n### Find column names\n\n\n::: {.cell}\n\n:::\n\n\n### Transform indicators\n\n-   What is the effect of exercise as measured by the NZAVS exercise scale on (1) depression and (2) anxiety.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# questions are:\n      # kessler_depressed,\n      # # During the last 30 days, how often did you feel so depressed that nothing could cheer you up?\n      # kessler_hopeless,\n      # # During the last 30 days, how often did you feel hopeless?\n      # kessler_nervous,\n      # # During the last 30 days, how often did you feel nervous?\n      # kessler_effort,\n      # # During the last 30 days, how often did you feel that everything was an effort?\n      # kessler_restless,\n      # # During the last 30 days, how often did you feel restless or fidgety ?\n      # kessler_worthless  # During the last 30 days, how often did you feel worthless?\ndt_start <- nzavs_synth %>%\n  arrange(id, wave) %>%\n  rowwise() %>%\n  mutate(\n    # see week 10 appendix 1 for a detailed explanation of how we obtain these 2 x factors\n    kessler_latent_depression = mean(\n      c(kessler_depressed, kessler_hopeless, kessler_effort),\n      na.rm = TRUE\n    ),\n    kessler_latent_anxiety  = mean(c(\n      kessler_effort, kessler_nervous, kessler_restless\n    ), na.rm = TRUE)\n  ) |>\n  ungroup() |>\n  # Coarsen 'hours_exercise' into categories\n  mutate(\n    hours_exercise_coarsen = cut(\n      hours_exercise,\n      # Hours spent exercising/ physical activity\n      breaks = c(-1, 3, 8, 200),\n      labels = c(\"inactive\",\n                 \"active\",\n                 \"very_active\"),\n      # Define thresholds for categories\n      levels = c(\"(-1,3]\", \"(3,8]\", \"(8,200]\"),\n      ordered = TRUE\n    )\n  ) |>\n  # Create a binary 'urban' variable based on the 'rural_gch2018' variable\n  mutate(urban = factor(\n    ifelse(\n      rural_gch2018 == \"medium_urban_accessibility\" |\n        # Define urban condition\n        rural_gch2018 == \"high_urban_accessibility\",\n      \"urban\",\n      # Label 'urban' if condition is met\n      \"rural\"  # Label 'rural' if condition is not met\n    )\n  ))\n```\n:::\n\n\n### Inspect your data\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# do some checks\nlevels(dt_start$hours_exercise_coarsen)\ntable(dt_start$hours_exercise_coarsen)\nmax(dt_start$hours_exercise)\nmin(dt_start$hours_exercise)\n# checks\n\n\n# justification for transforming exercise\" has a very long tail\nhist(dt_start$hours_exercise, breaks = 1000)\n# consider only those cases below < or = to 20\nhist(subset(dt_start, hours_exercise <= 20)$hours_exercise)\nhist(as.numeric(dt_start$hours_exercise_coarsen))\n```\n:::\n\n\n### Investigate assumption of positivity:\n\nRecall the positive assumption:\n\n**Positivity:** Can we intervene on the exposure at all levels of the covariates?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#These are the data by wave, but they don't track who changed.\n\n#  select only the baseline year and the exposure year.  That will give us change in the exposure. ()\ndt_exposure <- dt_start |>\n  \n  # select baseline year and exposure year\n  filter(wave == \"2018\" | wave == \"2019\") |>\n  \n  # select variables of interest\n  select(id, wave, hours_exercise_coarsen,  eth_cat) |>\n  \n  # the categorical variable needs to be numeric for us to use msm package to investigate change\n  mutate(hours_exercise_coarsen_n = as.numeric(hours_exercise_coarsen)) |>\n  droplevels()\n\n\n# check\n# dt_exposure |>\n#   tabyl(hours_exercise_coarsen_n, eth_cat,  wave )\n```\n:::\n\n\nI've written a function called `transition_table` that will help us assess change in the exposure at the *individual level.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#   consider people going from active to vary active\nout <- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure)\n\n\n# for a function I wrote to create state tables\nstate_names <- c(\"Inactive\", \"Somewhat Active\", \"Active\", \"Extremely Active\")\n\n# transition table\n\ntransition_table(out, state_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$explanation\n[1] \"This transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\"\n\n$table\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   2186   |      1324       |  295   |\n| Somewhat Active |   1019   |      2512       |  811   |\n|     Active      |   204    |       668       |  981   |\n```\n:::\n:::\n\n\nNext consider Māori only\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Maori only\ndt_exposure_maori <- dt_exposure |>\n  filter(eth_cat == \"māori\")\n\nout_m <- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_maori)\n\n# with this little support we might consider parametric models\nt_tab_m<- transition_table( out_m, state_names)\n\n#interpretation\ncat(t_tab_m$explanation)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n```\n:::\n\n```{.r .cell-code}\nprint(t_tab_m$table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   187    |       108       |   24   |\n| Somewhat Active |    92    |       188       |   61   |\n|     Active      |    28    |       58        |   75   |\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# filter euro\ndt_exposure_euro <- dt_exposure |>\n  filter(eth_cat == \"euro\")\n\n# model change\nout_e <- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_euro)\n\n# creat transition table.\nt_tab_e <- transition_table( out_e, state_names)\n\n#interpretation\ncat(t_tab_e$explanation)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n```\n:::\n\n```{.r .cell-code}\n# table\nprint(t_tab_e$table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   1843   |      1136       |  259   |\n| Somewhat Active |   870    |      2208       |  712   |\n|     Active      |   167    |       583       |  863   |\n```\n:::\n:::\n\n\nOverall we find evidence for change in the exposure variable. This suggest that we are ready to proceed with the next step of causal estimation.\n\n## Draw Dag\n\n\n::: {.cell codefolding='true'}\n\n```{.tikz .cell-code}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{shapes.geometric}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{decorations}\n\\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]\n\\tikzset{>=latex}\n\n\\begin{tikzpicture}[{every node/.append style}=draw]\n\\node [rectangle, draw=white] (U) at (0, 0) {U};\n\\node [rectangle, draw=black, align=left] (L) at (2, 0) {t0/L \\\\t0/A \\\\t0/Y};\n\\node [rectangle, draw=white] (A) at (4, 0) {t1/A};\n\\node [ellipse, draw=white] (Y) at (6, 0) {t2/Y};\n\\draw [-latex, draw=black] (U) to (L);\n\\draw [-latex, draw=black] (L) to (A);\n\\draw [-latex, draw=red, dotted] (A) to (Y);\n\\draw [-latex, bend left=50, draw =black] (L) to (Y);\n\\draw [-latex, bend right=50, draw =black, dotted] (U) to (Y);\n\\draw [-latex, bend left=50, draw =black, dotted] (U) to (A);\n\n\n\\end{tikzpicture}\n```\n\n\n::: {.cell-output-display}\n![Causal graph: three-wave panel design](12-content_files/figure-html/fig-dag-6-1.png){#fig-dag-6 width=100%}\n:::\n:::\n\n\n### Create wide data frame for analysis\n\nI've written a function\n\n\n::: {.cell table='wide_data'}\n\n```{.r .cell-code}\n############## ############## ############## ############## ############## ############## ############## ########\n####  ####  ####  CREATE DATA FRAME FOR ANALYSIS ####  ####  ################## ############## ######## #########\n############## ############## ############## ############## ############## ############## ############# #########\n\n\n# I have created a function that will put the data into the correct shape. Here are the steps.\n\n# Step 1: choose baseline variables (confounders).  here we select standard demographic variablees plus personality variables.\n\n# Note again that the function will automatically include the baseline exposure and basline outcome in the baseline variable confounder set so you don't need to include these. \n\n\n# here are some plausible baseline confounders\nbaseline_vars = c(\n  \"edu\",\n  \"male\",\n  \"eth_cat\",\n  \"employed\",\n  \"gen_cohort\",\n  \"nz_dep2018\",\n  \"nzsei13\",\n  \"partner\",\n  \"parent\",\n  \"pol_orient\",\n  \"urban\",\n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"honesty_humility\",\n  \"openness\",\n  \"neuroticism\",\n  \"modesty\",\n  \"religion_identification_level\"\n)\n\n\n## Step 2, select the exposure variable.  This is the \"cause\"\nexposure_var = c(\"hours_exercise_coarsen\")\n\n\n## step 3. select the outcome variable.  These are the outcomes.\noutcome_vars_reflective = c(\"kessler_latent_anxiety\",\n                            \"kessler_latent_depression\")\n\n\n\n# the function \"create_wide_data\" should be in your environment.\n# If not, make sure to run the first line of code in this script once more.  You may ignore the warnings. or uncomment and run the code below\n# source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\ndt_prepare <-\n  create_wide_data(\n    dat_long = dt_start,\n    baseline_vars = baseline_vars,\n    exposure_var = exposure_var,\n    outcome_vars = outcome_vars_reflective\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(exclude_vars)\n\n  # Now:\n  data %>% select(all_of(exclude_vars))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(t0_column_order)\n\n  # Now:\n  data %>% select(all_of(t0_column_order))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\n```\n:::\n\n```{.r .cell-code}\n# ignore warning\n```\n:::\n\n\n## Descriptive table\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# I have created a function that will allow you to take a data frame and\n# create a table\nbaseline_table(dt_prepare, output_format = \"markdown\")\n\n# but it is not very nice. Next up, is a better table\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# get data into shape\ndt_new <- dt_prepare %>%\n  select(starts_with(\"t0\")) %>%\n  rename_all( ~ stringr::str_replace(., \"^t0_\", \"\")) %>%\n  mutate(wave = factor(rep(\"baseline\", nrow(dt_prepare)))) |>\n  janitor::clean_names(case = \"screaming_snake\")\n\n\n# create a formula string\nbaseline_vars_names <- dt_new %>%\n  select(-WAVE) %>%\n  colnames()\n\ntable_baseline_vars <-\n  paste(baseline_vars_names, collapse = \"+\")\n\nformula_string_table_baseline <-\n  paste(\"~\", table_baseline_vars, \"|WAVE\")\n\ntable1::table1(as.formula(formula_string_table_baseline),\n               data = dt_new,\n               overall = FALSE)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"Rtable1\"><table class=\"Rtable1\">\n<thead>\n<tr>\n<th class='rowlabel firstrow lastrow'></th>\n<th class='firstrow lastrow'><span class='stratlabel'>baseline<br><span class='stratn'>(N=10000)</span></span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class='rowlabel firstrow'>EDU</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>5.85 (2.59)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>6.96 [-0.128, 10.1]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>MALE</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Male</td>\n<td>3905 (39.1%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Not_male</td>\n<td class='lastrow'>6095 (61.0%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>ETH_CAT</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>euro</td>\n<td>8641 (86.4%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>māori</td>\n<td>821 (8.2%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>pacific</td>\n<td>190 (1.9%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>asian</td>\n<td class='lastrow'>348 (3.5%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>EMPLOYED</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>0.836 (0.370)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>1.00 [0, 1.00]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>GEN_COHORT</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Gen_Silent: born< 1946</td>\n<td>166 (1.7%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>Gen Boomers: born >= 1946 & b.< 1965</td>\n<td>4257 (42.6%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>GenX: born >=1961 & b.< 1981</td>\n<td>3493 (34.9%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>GenY: born >=1981 & b.< 1996</td>\n<td>1883 (18.8%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>GenZ: born >= 1996</td>\n<td class='lastrow'>201 (2.0%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>NZ_DEP2018</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>4.46 (2.65)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>4.01 [0.835, 10.1]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>NZSEI13</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>57.0 (16.1)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>61.0 [9.91, 90.1]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>PARTNER</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>0.795 (0.404)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>1.00 [0, 1.00]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>PARENT</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>0.706 (0.456)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>1.00 [0, 1.00]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>POL_ORIENT</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>3.47 (1.40)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>3.09 [0.862, 7.14]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>URBAN</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>rural</td>\n<td>1738 (17.4%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>urban</td>\n<td class='lastrow'>8262 (82.6%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>AGREEABLENESS</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>5.36 (0.986)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>5.48 [0.977, 7.13]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>CONSCIENTIOUSNESS</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>5.19 (1.03)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>5.28 [0.938, 7.16]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>EXTRAVERSION</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>3.85 (1.21)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>3.80 [0.861, 7.07]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>HONESTY_HUMILITY</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>5.52 (1.12)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>5.71 [1.14, 7.15]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>OPENNESS</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>5.06 (1.10)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>5.12 [0.899, 7.15]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>NEUROTICISM</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>3.41 (1.17)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>3.31 [0.860, 7.08]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>MODESTY</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>6.07 (0.860)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>6.24 [2.17, 7.17]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>RELIGION_IDENTIFICATION_LEVEL</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>2.19 (2.07)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>1.00 [1.00, 7.00]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>HOURS_EXERCISE_COARSEN</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>inactive</td>\n<td>3805 (38.1%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>active</td>\n<td>4342 (43.4%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>very_active</td>\n<td class='lastrow'>1853 (18.5%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>KESSLER_LATENT_ANXIETY</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>1.16 (0.719)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>1.03 [-0.0800, 4.03]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>KESSLER_LATENT_DEPRESSION</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>0.744 (0.686)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>0.646 [-0.0871, 4.02]</td>\n</tr>\n</tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe need to do some more data wrangling, alas! Data wrangling is the majority of data analysis. The good news is that R makes wrangling relatively straightforward.\n\n1.  `mutate(id = factor(1:nrow(dt_prepare)))`: This creates a new column called `id` that has unique identification factors for each row in the dataset. It ranges from 1 to the number of rows in the dataset.\n\n2.  The next `mutate` operation is used to convert the `t0_eth_cat`, `t0_urban`, and `t0_gen_cohort` variables to factor type, if they are not already.\n\n3.  The `filter` command is used to subset the dataset to only include rows where the `t0_eth_cat` is either \"euro\" or \"māori\". The original dataset includes data with four different ethnic categories. This command filters out any row not related to these two groups.\n\n4.  `ungroup()` ensures that there's no grouping in the dataframe.\n\n5.  The `mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\"))` step standardizes all numeric columns in the dataset by subtracting the mean and dividing by the standard deviation (a z-score transformation). The resulting columns are renamed to include \"\\_z\" at the end of their original names.\n\n6.  The `select` function is used to keep only specific columns: the `id` column, any columns that are factors, and any columns that end in \"\\_z\".\n\n7.  The `relocate` functions re-order columns. The first `relocate` places the `id` column at the beginning. The next three `relocate` functions order the rest of the columns based on their names: those starting with \"t0\\_\" are placed before \"t1\\_\" columns, and those starting with \"t2\\_\" are placed after \"t1\\_\" columns.\n\n8.  `droplevels()` removes unused factor levels in the dataframe.\n\n9.  Finally, `skimr::skim(dt)` will print out a summary of the data in the `dt` object using the skimr package. This provides a useful overview of the data, including data types and summary statistics.\n\nThis function seems to be part of a data preparation pipeline in a longitudinal or panel analysis, where observations are ordered over time (indicated by t0\\_, t1\\_, t2\\_, etc.).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### ### ### ### ### ### SUBGROUP DATA ANALYSIS: DATA WRANGLING  ### ### ### ###\n\ndt <- dt_prepare|>\n  mutate(id = factor(1:nrow(dt_prepare))) |>\n  mutate(\n  t0_eth_cat = as.factor(t0_eth_cat),\n  t0_urban = as.factor(t0_urban),\n  t0_gen_cohort = as.factor(t0_gen_cohort)\n) |>\n  dplyr::filter(t0_eth_cat == \"euro\" |\n                t0_eth_cat == \"māori\") |> # Too few asian and pacific\n  ungroup() |>\n  # transform numeric variables into z scores (improves estimation)\n  dplyr::mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) %>%\n  # select only factors and numeric values that are z-scores\n  select(id, # category is too sparse\n         where(is.factor),\n         ends_with(\"_z\"), ) |>\n  # tidy data frame so that the columns are ordered by time (useful for more complex models)\n  relocate(id, .before = starts_with(\"t1_\"))   |>\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\"))  |>\n  relocate(starts_with(\"t2_\"), .after = starts_with(\"t1_\")) |>\n  droplevels()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# checks\nhist(dt$t2_kessler_latent_depression_z)\nhist(dt$t2_kessler_latent_anxiety_z)\n\ndt |>\n  tabyl(t0_eth_cat, t1_hours_exercise_coarsen ) |>\n  kbl(format = \"markdown\")\n\n# Visualise missingness\nnaniar::vis_miss(dt)\n\n# save your dataframe for future use\n\n# make dataframe\ndt = as.data.frame(dt)\n\n# save data\nsaveRDS(dt, here::here(\"data\", \"dt\"))\n```\n:::\n\n\n### Calculate propensity scores\n\nNext we generate propensity scores.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read  data -- you may start here if you need to repeat the analysis\ndt <- readRDS(here::here(\"data\", \"dt\"))\n\n# get column names\nbaseline_vars_reflective_propensity <- dt|>\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |> colnames()\n\n# define our exposure\nX <- \"t1_hours_exercise_coarsen\"\n\n# define subclasses\nS <- \"t0_eth_cat\"\n\n# Make sure data is in a data frame format\ndt <- data.frame(dt)\n\n\n# next we use our trick for creating a formula string, which will reduce our work\nformula_str_prop <-\n  paste(X,\n        \"~\",\n        paste(baseline_vars_reflective_propensity, collapse = \"+\"))\n\n# this shows the exposure variable as predicted by the baseline confounders.\nformula_str_prop\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"t1_hours_exercise_coarsen ~ t0_male+t0_gen_cohort+t0_urban+t0_hours_exercise_coarsen+t0_edu_z+t0_employed_z+t0_nz_dep2018_z+t0_nzsei13_z+t0_partner_z+t0_parent_z+t0_pol_orient_z+t0_agreeableness_z+t0_conscientiousness_z+t0_extraversion_z+t0_honesty_humility_z+t0_openness_z+t0_neuroticism_z+t0_modesty_z+t0_religion_identification_level_z+t0_kessler_latent_anxiety_z+t0_kessler_latent_depression_z\"\n```\n:::\n:::\n\n\nFor propensity score analysis, we will try several different approaches. We will want to select the method that produces the best balance.\n\nI typically use `ps` (classical propensity scores), `ebal` and `energy`. The latter two in my experience yeild good balance. Also `energy` will work with *continuous* exposures.\n\nFor more information, see <https://ngreifer.github.io/WeightIt/>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# traditional propensity scores-- note we select the ATT and we have a subgroup \ndt_match_ps <- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ps\"\n)\n\nsaveRDS(dt_match_ps, here::here(\"data\", \"dt_match_ps\"))\n\n\n# ebalance\ndt_match_ebal <- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ebal\"\n)\n\n# save output\nsaveRDS(dt_match_ebal, here::here(\"data\", \"dt_match_ebal\"))\n\n\n\n## energy balance method\ndt_match_energy <- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  #focal = \"high\", # for use with ATT\n  method = \"energy\"\n)\nsaveRDS(dt_match_energy, here::here(\"data\", \"dt_match_energy\"))\n```\n:::\n\n\nResults, first for Europeans\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#dt_match_energy <- readRDS(here::here(\"data\", \"dt_match_energy\"))\ndt_match_ebal <- readRDS(here::here(\"data\", \"dt_match_ebal\"))\n#dt_match_ps <- readRDS(here::here(\"data\", \"dt_match_ps\"))\n\n# next we inspect balance. \"Max.Diff.Adj\" should ideally be less than .05, but less than .1 is ok. This is the standardised mean difference. The variance ratio should be less than 2. \n# note that if the variables are unlikely to influence the outcome we can be less strict. \n\n#See: Hainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n# Cole SR, Hernan MA. Constructing inverse probability weights for marginal structural models. American Journal of\n# Epidemiology 2008; 168(6):656–664.\n\n# Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies\n# Peter C. Austin, Elizabeth A. Stuart\n# https://onlinelibrary.wiley.com/doi/10.1002/sim.6607\n\n#bal.tab(dt_match_energy$euro)   #  good\nbal.tab(dt_match_ebal$euro)   #  best\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0001\nt0_gen_cohort_Gen_Silent: born< 1946                Binary       0.0001\nt0_gen_cohort_Gen Boomers: born >= 1946 & b.< 1965  Binary       0.0001\nt0_gen_cohort_GenX: born >=1961 & b.< 1981          Binary       0.0001\nt0_gen_cohort_GenY: born >=1981 & b.< 1996          Binary       0.0001\nt0_gen_cohort_GenZ: born >= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0001\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0003\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0001\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0000\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0001\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0001\nt0_modesty_z                                       Contin.       0.0001\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0001\nt0_kessler_latent_depression_z                     Contin.       0.0000\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1855.89 3659.59     1052.01\n```\n:::\n\n```{.r .cell-code}\n#bal.tab(dt_match_ps$euro)   #  not as good\n\n# here we show only the best tab, but you should put all information into an appendix\n```\n:::\n\n\nResults for Maori\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# who only Ebal\n#bal.tab(dt_match_energy$māori)   #  good\nbal.tab(dt_match_ebal$māori)   #  best\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0000\nt0_gen_cohort_Gen_Silent: born< 1946                Binary       0.0000\nt0_gen_cohort_Gen Boomers: born >= 1946 & b.< 1965  Binary       0.0000\nt0_gen_cohort_GenX: born >=1961 & b.< 1981          Binary       0.0000\nt0_gen_cohort_GenY: born >=1981 & b.< 1996          Binary       0.0000\nt0_gen_cohort_GenZ: born >= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0000\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0001\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0002\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0001\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0000\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0000\nt0_modesty_z                                       Contin.       0.0000\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0000\nt0_kessler_latent_depression_z                     Contin.       0.0001\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   307.   354.        160.  \nAdjusted     220.54 321.09       76.39\n```\n:::\n\n```{.r .cell-code}\n#bal.tab(dt_match_ps$māori)   #  not good\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# code for summar\nsum_e <- summary(dt_match_ebal$euro)\nsum_m <- summary(dt_match_ebal$māori)\n\n# summary euro\nsum_e\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Summary of weights\n\n- Weight ranges:\n\n               Min                                  Max\ninactive    0.2310 |---------------------------| 7.0511\nactive      0.5769  |----|                       1.9603\nvery_active 0.1601 |----------------------|      5.9191\n\n- Units with the 5 most extreme weights by group:\n                                               \n               6560      9   7209   4878   5105\n    inactive 5.1084 5.1312 5.1642 5.3517 7.0511\n               3279   1867   4754   2783   7057\n      active 1.7467 1.7654 1.7701 1.8692 1.9603\n               5977   4293    700   2352   4765\n very_active 5.1495 5.3064 5.4829 5.7273 5.9191\n\n- Weight statistics:\n\n            Coef of Var   MAD Entropy # Zeros\ninactive          0.743 0.536   0.212       0\nactive            0.270 0.248   0.036       0\nvery_active       0.862 0.637   0.302       0\n\n- Effective Sample Sizes:\n\n           inactive  active very_active\nUnweighted  2880.   3927.       1834.  \nWeighted    1855.89 3659.59     1052.01\n```\n:::\n\n```{.r .cell-code}\n# summary maori\nsum_m\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Summary of weights\n\n- Weight ranges:\n\n               Min                                  Max\ninactive    0.2213  |---------------|            3.8101\nactive      0.3995   |-----|                     1.9800\nvery_active 0.0719 |---------------------------| 6.2941\n\n- Units with the 5 most extreme weights by group:\n                                               \n                296    322    355    758    812\n    inactive 3.0104  3.407 3.6372 3.7101 3.8101\n                 95    783    473    703    699\n      active 1.8319 1.8387 1.9395 1.9436   1.98\n                745    149     78    226    718\n very_active 4.0921 4.3405 4.4111 4.6833 6.2941\n\n- Weight statistics:\n\n            Coef of Var   MAD Entropy # Zeros\ninactive          0.627 0.475   0.170       0\nactive            0.321 0.264   0.050       0\nvery_active       1.050 0.732   0.411       0\n\n- Effective Sample Sizes:\n\n           inactive active very_active\nUnweighted   307.   354.        160.  \nWeighted     220.54 321.09       76.39\n```\n:::\n:::\n\n::: {.cell .column-page-right}\n\n```{.r .cell-code}\nlove_plot_e <- love.plot(dt_match_ebal$euro,\n          binary = \"std\",\n          thresholds = c(m = .1))+ labs(title = \"NZ Euro Weighting: method e-balance\")\n\n# plot\nlove_plot_e \n```\n\n::: {.cell-output-display}\n![](12-content_files/figure-html/love_plot_euro-1.png){width=672}\n:::\n:::\n\n::: {.cell .column-page-right}\n\n```{.r .cell-code}\nlove_plot_m <- love.plot(dt_match_ebal$māori,\n          binary = \"std\",\n          thresholds = c(m = .1)) + labs(title = \"Māori Weighting: method e-balance\")\n# plot\nlove_plot_m\n```\n\n::: {.cell-output-display}\n![](12-content_files/figure-html/love_plot_maori-1.png){width=672}\n:::\n:::\n\n\n### Example Summary NZ Euro Propensity scores.\n\n> We estimated propensity score analysis using entropy balancing, energy balancing and traditional propensity scores. Of these approaches, entropy balancing provided the best balance. The results indicate an excellent balance across all variables, with Max.Diff.Adj values significantly below the target threshold of 0.05 across a range of binary and continuous baseline confounders, including gender, generation cohort, urban_location, exercise hours (coarsened, baseline), education, employment status, depression, anxiety, and various personality traits. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs.\n\n> The effective sample sizes were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 2880, 3927, and 1834, respectively. After adjustment, the effective sample sizes were reduced to 1855.89, 3659.59, and 1052.01, respectively.\n\n> The weight ranges for the inactive, active, and very active groups varied, with the inactive group showing the widest range (0.2310 to 7.0511) and the active group showing the narrowest range (0.5769 to 1.9603). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\n\n> We also identified the units with the five most extreme weights by group. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\n\n> We plotted these results using love plots, visually confirming both the balance in the propensity score model using entropy balanced weights, and the imbalance in the model that does not adjust for baseline confounders.\n\n> Overall, these findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, conditional on the measured covariates included in the model.\n\n### Example Summary Maori Propensity scores.\n\nResults:\n\n> The entropy balancing method was also the best performing method that was applied to a subgroup analysis of the Māori population. Similar to the NZ European subgroup analysis, the method achieved a high level of balance across all treatment pairs for the Māori subgroup. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs for the Māori subgroup.\n\n> The effective sample sizes for the Māori subgroup were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 307, 354, and 160, respectively. After adjustment, the effective sample sizes were reduced to 220.54, 321.09, and 76.39, respectively\n\n> The weight ranges for the inactive, active, and very active groups in the Māori subgroup varied, with the inactive group showing the widest range (0.2213 to 3.8101) and the active group showing the narrowest range (0.3995 to 1.9800). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\n\n> The study also identified the units with the five most extreme weights by group for the Māori subgroup. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\n\n> In conclusion, the results of the Māori subgroup analysis are consistent with the overall analysis. The entropy balancing method achieved a high level of balance across all treatment pairs, with Max.Diff.Adj values significantly below the target threshold. These findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, even in subgroup analyses.\n\n### More data wrangling\n\nNote that we need to attach the `weights` from the propensity score model back to the data.\n\nHowever, because our weighting analysis estimates a model for the *exposure*, we only need to do this analysis once, no matter how many outcomes we investigate. So there's a little good news.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prepare nz_euro data\ndt<- readRDS(here::here(\"data\", \"dt\")) # original data subset only nz europeans\n\ndt_ref_e <- subset(dt, t0_eth_cat == \"euro\") # original data subset only nz europeans\n\n# add weights\ndt_ref_e$weights <- dt_match_ebal$euro$weights # get weights from the ps matching model,add to data\n\n# prepare maori data\ndt_ref_m <- subset(dt, t0_eth_cat == \"māori\")# original data subset only maori\n\n# add weights\ndt_ref_m$weights <- dt_match_ebal$māori$weights # get weights from the ps matching model, add to data\n\n# combine data into one data frame\ndt_ref_all <- rbind(dt_ref_e, dt_ref_m) # combine the data into one dataframe. \n\n# save data for later use, if needed\nsaveRDS(dt_ref_all, here::here(\"data\",\"dt_ref_all\"))\n```\n:::\n\n\n## Anxiety Analysis and Results\n\nThis is the analysis code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we do not evaluate to save time\n### SUBGROUP analysis\ndf <-  dt_ref_all\nY <-  \"t2_kessler_latent_anxiety_z\"\nX <- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str <-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n  # formula_str. # inspect on our own time \n\n\n\n# fit model\nfit_all_all  <- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all <- sim(fit_all_all, n = nsims, vcov = \"HC0\")\n\n# simulate effect as modified in europeans\nsim_estimand_all_e <- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = TRUE\n)\n\n#rm(sim_estimand_all_e)\n# note contrast of interest\nsim_estimand_all_e <-\n  transform(sim_estimand_all_e, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n#rm(sim_estimand_all_m)\n\n# simulate effect as modified in māori\nsim_estimand_all_m <- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = TRUE\n)\n\n# combine\n#m(sim_estimand_all_m)\n\nsim_estimand_all_m <-\n  transform(sim_estimand_all_m, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n# rearrange\nnames(sim_estimand_all_e) <-\n  paste(names(sim_estimand_all_e), \"e\", sep = \"_\")\n\n\nnames(sim_estimand_all_m) <-\n  paste(names(sim_estimand_all_m), \"m\", sep = \"_\")\n\nsummary( sim_estimand_all_e )\n\nest_all_anxiety <- cbind(sim_estimand_all_m, sim_estimand_all_e)\nest_all_anxiety <- transform(est_all_anxiety, `RD_m - RD_e` = RD_m - RD_e)\n\nsaveRDS(sim_estimand_all_e, here::here(\"data\",\"sim_estimand_all_e\"))\nsaveRDS(sim_estimand_all_m, here::here(\"data\",\"sim_estimand_all_m\"))\nsaveRDS(est_all_anxiety, here::here(\"data\",\"est_all_anxiety\"))\n```\n:::\n\n\n## Table of Subgroup Results plus Evalues\n\n\n\n<!-- > E[Y(1)]-E[Y(0)] presents the causal contrast between the expected value of the exposure were all within the target population {Group Var Here} to receive the exposure E[Y(1)] in comparison to the expected value of the exposure were to recieve the baseline treatment  E[Y(0)].  -->\n\n<!-- > The Confidence interval for these estimates is between {2.5% column here} at the lower bound and {97.5% value here at the uppoer bound}.  -->\n\n\n<!-- > E-values represent the minimum strength or magnitude of association that an unmeasured confounder would need to have with both treatment and outcome in order to explain the observed effect estimates conditional on measured covariates [@vanderweele2017]. -->\n\n<!-- > With an observed risk ratio of RR = {E_Value Here}, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of {E_VAL_bound}-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of {EVALUE HERE} - fold each could do so, but weaker joint confounder associations could not..[@vanderweele2017] -->\n\n\nI've created functions for reporting results so you can use this code, changing it to suit your analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# return stored estimates \nsim_estimand_all_e <- readRDS(here::here(\"data\",\"sim_estimand_all_e\"))\nsim_estimand_all_m<- readRDS(here::here(\"data\",\"sim_estimand_all_m\"))\n\n# create individual summaries \nsum_e <- summary(sim_estimand_all_e)\nsum_m <- summary(sim_estimand_all_m)\n\n\n# create individual tables\ntab_e <- sub_tab_ate(sum_e, new_name = \"NZ Euro Anxiety\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 4)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n```\n:::\n\n```{.r .cell-code}\ntab_m <- sub_tab_ate(sum_m, new_name = \"Māori Anxiety\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nConfidence interval crosses the true value, so its E-value is 1.\n```\n:::\n\n```{.r .cell-code}\n# expand tables \nplot_e <- sub_group_tab(tab_e, type= \"RD\")\nplot_m <- sub_group_tab(tab_m, type= \"RD\")\n\nbig_tab <- rbind(plot_e,plot_m)\n\n\n# table for anxiety outcome --format as \"markdown\" if you are using quarto documents\nbig_tab |> \n  kbl(format=\"markdown\")\n```\n\n::: {.cell-output-display}\n|group           | E[Y(1)]-E[Y(0)]|  2.5 %| 97.5 %| E_Value| E_Val_bound|Estimate   |estimate_lab                            |\n|:---------------|---------------:|------:|------:|-------:|-----------:|:----------|:---------------------------------------|\n|NZ Euro Anxiety |          -0.077| -0.131| -0.022|   1.352|       1.167|negative   |-0.077 (-0.131--0.022) [EV 1.352/1.167] |\n|Māori Anxiety   |           0.027| -0.114|  0.188|   1.183|       1.000|unreliable |0.027 (-0.114-0.188) [EV 1.183/1]       |\n:::\n:::\n\n\n\n\n## Graph of the result\n\nI've create a function you can use to graph your results. Here is the code, adjust to suit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# group tables\nsub_group_plot_ate(big_tab, title = \"Effect of Exercise on Anxiety\", subtitle = \"Subgroup Analysis: NZ Euro and Māori\", xlab = \"Groups\", ylab = \"Effects\",\n                 x_offset = -1,\n                           x_lim_lo = -1,\n                           x_lim_hi = 1.5)\n```\n\n::: {.cell-output-display}\n![](12-content_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n### Report the anxiety result.\n\n\n> For the New Zealand European group, our results suggest that exercise potentially reduces anxiety, with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of -0.077. The associated confidence interval, ranging from -0.131 to -0.022, does not cross zero, providing more certainty in our estimate.\n\n> E-values quantify the minimum strength of association that an unmeasured confounding variable would need to have with both the treatment and outcome, to fully explain away our observed effect. In this case, any unmeasured confounder would need to be associated with both exercise and anxiety reduction, with a risk ratio of at least 1.352 to explain away the observed effect, and at least 1.167 to shift the confidence interval to include a null effect.\n\n> Turning to the Māori group, the data suggest a possible reducing effect of exercise on anxiety, with a causal contrast value of 0.027. Yet, the confidence interval for this estimate (-0.114 to 0.188) also crosses zero, indicating similar uncertainties. An unmeasured confounder would need to have a risk ratio of at least 1.183 with both exercise and anxiety to account for our observed effect, and a risk ratio of at least 1 to render the confidence interval inclusive of a null effect.\n\n> Thus, while our analysis suggests that exercise could potentially reduce anxiety in both New Zealand Europeans and Māori, we advise caution in interpretation. The confidence intervals crossing zero reflect substantial uncertainties, and the possible impact of unmeasured confounding factors further complicates the picture.\n\n\nHere's a function that will do much of this work for you. However, you'll need to adjust it, and supply your own interpretation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|label: interpretation function\n#| eval: false\ninterpret_results_subgroup <- function(df, outcome, exposure) {\n  df <- df %>%\n    mutate(\n      report = case_when(\n        E_Val_bound > 1.2 & E_Val_bound < 2 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests stronger confidence in our findings.\"\n        ),\n        E_Val_bound >= 2 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"With an observed risk ratio of RR = \", E_Value, \", an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each could do so, but weaker joint confounder associations could not. Here we find stronger evidence that the result is robust to unmeasured confounding.\"\n        ),\n        E_Val_bound < 1.2 & E_Val_bound > 1 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests we should interpret these findings with caution given uncertainty in the model.\"\n        ),\n        E_Val_bound == 1 ~ paste0(\n          \"For the \", group, \", the data suggests a potential effect of \", exposure, \" on \", outcome, \", with a causal contrast value of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"However, the confidence interval for this estimate, ranging from \", `2.5 %`,\" to \", `97.5 %`, \", crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the \", outcome, \" and the \", exposure, \" by a risk ratio of \", E_Value, \" could explain away the observed associations, even after accounting for the measured confounders. \",\n          \"This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of \", exposure, \" on \", outcome, \" for the \", group, \", the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n        )\n      )\n    )\n  return(df$report)\n}\n```\n:::\n\n\n\nYou run the function like this: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninterpret_results_subgroup(big_tab, outcome = \"Anxiety\", exposure = \"Excercise\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"For the NZ Euro Anxiety, our results suggest that Excercise may potentially influence Anxiety, with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of -0.077.\\nThe associated confidence interval, ranging from -0.131 to -0.022, does not cross zero, providing more certainty in our estimate. The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of 1.352 with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of 1.167 to shift the confidence interval to include the null effect. This suggests we should interpret these findings with caution given uncertainty in the model.\"                                                                                                                                      \n[2] \"For the Māori Anxiety, the data suggests a potential effect of Excercise on Anxiety, with a causal contrast value of 0.027.\\nHowever, the confidence interval for this estimate, ranging from -0.114 to 0.188, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Anxiety and the Excercise by a risk ratio of 1.183 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Excercise on Anxiety for the Māori Anxiety, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n```\n:::\n:::\n\n\n\nEasy!\n\n\n\n### Estimate the subgroup contrast\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculated above\nest_all_anxiety <- readRDS( here::here(\"data\",\"est_all_anxiety\"))\n\n# make the sumamry into a dataframe so we can make a table\ndf <- as.data.frame(summary(est_all_anxiety))\n\n# get rownames for selecting the correct row\ndf$RowName <- row.names(df)\n\n# select the correct row -- the group contrast\nfiltered_df <- df |> \n  dplyr::filter(RowName == \"RD_m - RD_e\") \n\n\n# pring the filtered data frame\nlibrary(kableExtra)\nfiltered_df  |> \n  select(-RowName) |> \n  kbl(digits = 3) |> \n  kable_material(c(\"striped\", \"hover\")) \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-material lightable-striped lightable-hover\" style='font-family: \"Source Sans Pro\", helvetica, sans-serif; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n   <th style=\"text-align:right;\"> 2.5 % </th>\n   <th style=\"text-align:right;\"> 97.5 % </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> RD_m - RD_e </td>\n   <td style=\"text-align:right;\"> 0.104 </td>\n   <td style=\"text-align:right;\"> -0.042 </td>\n   <td style=\"text-align:right;\"> 0.279 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAnother option for making the table using markdown. This would be useful if you were writing your article using qaurto. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiltered_df  |> \n  select(-RowName) |> \n  kbl(digits = 3, format = \"markdown\")\n```\n\n::: {.cell-output-display}\n|            | Estimate|  2.5 %| 97.5 %|\n|:-----------|--------:|------:|------:|\n|RD_m - RD_e |    0.104| -0.042|  0.279|\n:::\n:::\n\n\nReport result along the following lines:\n\n> The estimated reduction of anxiety from exercise is higher overall for New Zealand Europeans (RD_e) compared to Māori (RD_m). This is indicated by the estimated risk difference (RD_m - RD_e) of 0.104. However, there is uncertainty in this estimate, as the confidence interval (-0.042 to 0.279) crosses zero. This indicates that we cannot be confident that the difference in anxiety reduction between New Zealand Europeans and Māori is reliable. It's possible that the true difference could be zero or even negative, suggesting higher anxiety reduction for Māori. Thus, while there's an indication of higher anxiety reduction for New Zealand Europeans, the uncertainty in the estimate means we should interpret this difference with caution.\n\n\n\n\n\n## Depression Analysis and Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### SUBGROUP analysis\ndt_ref_all <- readRDS(here::here(\"data\", \"dt_ref_all\"))\n# get column names\nbaseline_vars_reflective_propensity <- dt|>\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |> colnames()\ndf <-  dt_ref_all\nY <-  \"t2_kessler_latent_depression_z\"\nX <- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str <-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# fit model\nfit_all_dep  <- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n\n# coefs <- coef(fit_all_dep)\n# table(is.na(coefs))#   \n# insight::get_varcov(fit_all_all)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all <- sim(fit_all_dep, n = nsims, vcov = \"HC1\")\n\n\n# simulate effect as modified in europeans\nsim_estimand_all_e_d <- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = TRUE)\n\n\n# note contrast of interest\nsim_estimand_all_e_d <-\n  transform(sim_estimand_all_e_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n\n# simulate effect as modified in māori\nsim_estimand_all_m_d <- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = TRUE\n)\n\n# combine\nsim_estimand_all_m_d <-\n  transform(sim_estimand_all_m_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n\n# summary\n#summary(sim_estimand_all_e_d)\n#summary(sim_estimand_all_m_d)\n\n# rearrange\nnames(sim_estimand_all_e_d) <-\n  paste(names(sim_estimand_all_e_d), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m_d) <-\n  paste(names(sim_estimand_all_m_d), \"m\", sep = \"_\")\n\n\nest_all_d <- cbind(sim_estimand_all_m_d, sim_estimand_all_e_d)\nest_all_d <- transform(est_all_d, `RD_m - RD_e` = RD_m - RD_e)\nsaveRDS(sim_estimand_all_m_d, here::here(\"data\", \"sim_estimand_all_m_d\"))\nsaveRDS(sim_estimand_all_e_d, here::here(\"data\", \"sim_estimand_all_e_d\"))\n```\n:::\n\n\n### Report anxiety results\n\n::: {.cell}\n\n```{.r .cell-code}\n# return stored estimates \nsim_estimand_all_e_d <- readRDS(here::here(\"data\",\"sim_estimand_all_e_d\"))\nsim_estimand_all_m_d<- readRDS(here::here(\"data\",\"sim_estimand_all_m_d\"))\n\n# create individual summaries \nsum_e_d <- summary(sim_estimand_all_e_d)\nsum_m_d <- summary(sim_estimand_all_m_d)\n\n\n# create individual tables\ntab_ed <- sub_tab_ate(sum_e_d, new_name = \"NZ Euro Depression\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nConfidence interval crosses the true value, so its E-value is 1.\n```\n:::\n\n```{.r .cell-code}\ntab_md <- sub_tab_ate(sum_m_d, new_name = \"Māori Depression\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nConfidence interval crosses the true value, so its E-value is 1.\n```\n:::\n\n```{.r .cell-code}\n# expand tables \nplot_ed <- sub_group_tab(tab_ed, type= \"RD\")\nplot_md <- sub_group_tab(tab_md, type= \"RD\")\n\nbig_tab_d <- rbind(plot_ed,plot_md)\n\n\n# table for anxiety outcome --format as \"markdown\" if you are using quarto documents\nbig_tab_d |> \n  kbl(format=\"markdown\")\n```\n\n::: {.cell-output-display}\n|group              | E[Y(1)]-E[Y(0)]|  2.5 %| 97.5 %| E_Value| E_Val_bound|Estimate   |estimate_lab                       |\n|:------------------|---------------:|------:|------:|-------:|-----------:|:----------|:----------------------------------|\n|NZ Euro Depression |          -0.039| -0.099|  0.019|   1.231|           1|unreliable |-0.039 (-0.099-0.019) [EV 1.231/1] |\n|Māori Depression   |           0.028| -0.125|  0.176|   1.190|           1|unreliable |0.028 (-0.125-0.176) [EV 1.19/1]   |\n:::\n:::\n\n\n\n### Graph depression result\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# group tables\nsub_group_plot_ate(big_tab_d, title = \"Effect of Exercise on Depression\", subtitle = \"Subgroup Analysis: NZ Euro and Māori\", xlab = \"Groups\", ylab = \"Effects\",\n                 x_offset = -1,\n                           x_lim_lo = -1,\n                           x_lim_hi = 1.5)\n```\n\n::: {.cell-output-display}\n![](12-content_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n###  Interpretation\n\nUse the function, again, modify the outputs to fit with your study and results and provide your own interpretation. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninterpret_results_subgroup(big_tab_d, exposure = \"Exercise\", outcome = \"Depression\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"For the NZ Euro Depression, the data suggests a potential effect of Exercise on Depression, with a causal contrast value of -0.039.\\nHowever, the confidence interval for this estimate, ranging from -0.099 to 0.019, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Depression and the Exercise by a risk ratio of 1.231 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Exercise on Depression for the NZ Euro Depression, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n[2] \"For the Māori Depression, the data suggests a potential effect of Exercise on Depression, with a causal contrast value of 0.028.\\nHowever, the confidence interval for this estimate, ranging from -0.125 to 0.176, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Depression and the Exercise by a risk ratio of 1.19 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Exercise on Depression for the Māori Depression, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"      \n```\n:::\n:::\n\n\n### Estimate the subgroup contrast\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculated above\nest_all_d <- readRDS( here::here(\"data\",\"est_all_d\"))\n\n# make the sumamry into a dataframe so we can make a table\ndfd <- as.data.frame(summary(est_all_d))\n\n# get rownames for selecting the correct row\ndfd$RowName <- row.names(dfd)\n\n# select the correct row -- the group contrast\nfiltered_dfd <- dfd |> \n  dplyr::filter(RowName == \"RD_m - RD_e\") \n\n\n# Print the filtered data frame\nlibrary(kableExtra)\nfiltered_dfd  |> \n  select(-RowName) |> \n  kbl(digits = 3) |> \n  kable_material(c(\"striped\", \"hover\")) \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-material lightable-striped lightable-hover\" style='font-family: \"Source Sans Pro\", helvetica, sans-serif; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n   <th style=\"text-align:right;\"> 2.5 % </th>\n   <th style=\"text-align:right;\"> 97.5 % </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> RD_m - RD_e </td>\n   <td style=\"text-align:right;\"> 0.068 </td>\n   <td style=\"text-align:right;\"> -0.09 </td>\n   <td style=\"text-align:right;\"> 0.229 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nReporting might be: \n\n> The estimated reduction of depression from exercise is higher overall for New Zealand Europeans (RD_e) compared to Māori (RD_m). This is suggested by the estimated risk difference (RD_m - RD_e) of 0.068. However, there is a degree of uncertainty in this estimate, as the confidence interval (-0.09 to 0.229) crosses zero. This suggests that we cannot be confident that the difference in depression reduction between New Zealand Europeans and Māori is statistically significant. It's possible that the true difference could be zero or even negative, implying a greater depression reduction for Māori than New Zealand Europeans. Thus, while the results hint at a larger depression reduction for New Zealand Europeans, the uncertainty in this estimate urges us to interpret this difference with caution.\n\n\n\n### Discusion \n\nYou'll need to write the discussion for yourself. Here's a start: \n\n\n> In our study, we employed a robust statistical method that helps us estimate the impact of exercise on reducing anxiety among different population groups – New Zealand Europeans and Māori. This method has the advantage of providing reliable results even if our underlying assumptions aren't entirely accurate – a likely scenario given the complexity of real-world data.  However, this robustness comes with a trade-off: it gives us wider ranges of uncertainty in our estimates. This doesn't mean the analysis is flawed; rather, it accurately represents our level of certainty given the data we have.\n\n#### **Exercise and anxiety**\n\n> Our analysis suggests that exercise may have a greater effect in reducing anxiety among New Zealand Europeans compared to Māori. This conclusion comes from our primary causal estimate, the risk difference, which is 0.104. However, it's crucial to consider our uncertainty in this value. We represent this uncertainty as a range, also known as a confidence interval. In this case, the interval ranges from -0.042 to 0.279. What this means is, given our current data and method, the true effect could plausibly be anywhere within this range. While our best estimate shows a higher reduction in anxiety for New Zealand Europeans, the range of plausible values includes zero and even negative values. This implies that the true effect could be no difference between the two groups or even a higher reduction in Māori. Hence, while there's an indication of a difference, we should interpret it cautiously given the wide range of uncertainty.\n\n> Thus, although our analysis points towards a potential difference in how exercise reduces anxiety among these groups, the level of uncertainty means we should be careful about drawing firm conclusions. More research is needed to further explore these patterns.\n\n#### **Exercise and depression**\n\n\n> In addition to anxiety, we also examined the effect of exercise on depression. We do not find evidence for reduction of depression from exercise in either group. We do not find evidence for the effect of weekly exercise -- as self-reported -- on depression. \n\n#### **Limitations**\n\n> It is important to bear in mind that statistical results are only one piece of a larger scientific puzzle about the relationship between excercise and well-being. Other pieces include understanding the context, incorporating subject matter knowledge, and considering the implications of the findings. In the present study, wide confidence intervals suggest the possibility of considerable individual differences.$\\dots$ nevertheless, $\\dots$\n\n\n**Again, you will need to come up with your own discussion, but you may follow the step-by-step instructions above as a guide.**\n\n",
    "supporting": [
      "12-content_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/table1-1.0/table1_defaults.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}